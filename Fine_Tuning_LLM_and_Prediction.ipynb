{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A4A68kO79zW"
   },
   "source": [
    "## Fine-Tuning LLM and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85SitHrx7_ok"
   },
   "source": [
    "### Setup:\n",
    "- Install USC VPN: https://itservices.usc.edu/vpn/\n",
    "- Go to https://ondemand.carc.usc.edu/pun/sys/dashboard and login\n",
    "- Start JupyterLab by clicking on the JupyterLab icon, fill out the info, and click on launch. Here's an example of working parameters. You may need to change the account to which resources are accounted for:\n",
    "    * Cluster: Discovery\n",
    "    * JupyterLab version: 4.0.5\n",
    "    * Modules to load (optional): gcc/11.3.0 python/3.9.12 git\n",
    "    * Account: ll_774_951 #Change the account with the one from which the resources will be charged\n",
    "    * Partition: main\n",
    "    * Number of CPUs: 1\n",
    "    * Memory (GB): 16\n",
    "    * Number of hours: 2\n",
    "- Once Jupyter starts, click on the icon on the top left to upload files and select this notebook. The notebook will appear in the file list on the left column. Open the notebook just uploaded by double-clicking on it\n",
    "- Install the hiyouga/LLaMA-Efficient-Tuning repo by running the next code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BClQmMlA8Cww"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Efficient-Tuning'...\n",
      "remote: Enumerating objects: 2669, done.\u001b[K\n",
      "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 2669 (delta 24), reused 55 (delta 20), pack-reused 2601\u001b[K\n",
      "Receiving objects: 100% (2669/2669), 173.98 MiB | 13.62 MiB/s, done.\n",
      "Resolving deltas: 100% (1831/1831), done.\n",
      "Updating files: 100% (108/108), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0AhT13M8Bxm"
   },
   "source": [
    "### Generate and upload train and test sets:\n",
    "- Generate a `train.json` and a `test.json` to train and test the model. The JSONs need this structure:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Is the text of this tweet an information operation: iPhone'da parmak izi uyarısıhttp://t.co/ETXCTbgWcR?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"True\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Is the text of this tweet an information operation: @libanews Israël fournit-il des armes à Al-Qaïda en Syrie ? http://t.co/q3UfgvSSaO?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"True\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "- Open a new browser tab, go to https://ondemand.carc.usc.edu/pun/sys/dashboard, and click on \"Files\" in the top bar, then \"Home Directory.\" Then click and open the `LLaMA-Efficient-Tuning` folder, then the `data` folder\n",
    "- Click on the blue \"Upload\" button to upload the JSON files you just created\n",
    "- Specify the LLaMA-Efficient-Tuning folder you created earlier by cloning the repository, and the JSON files you want you want the model to be aware of\n",
    "- Run the code following code to update the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7p-_S9Y98Gll"
   },
   "outputs": [],
   "source": [
    "efficient_finetuning_folder = \"/home1/pante/UntitledFolder/LLaMA-Efficient-Tuning\" #absolute path\n",
    "train = \"train.json\"\n",
    "test = \"test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "I5Vt45TK8JB9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added train to data_info.json\n",
      "Added test to data_info.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def add_json_file(efficient_finetuning_folder, json_file_name):\n",
    "    # Replace {username} with your actual username\n",
    "    data_info_file = f\"{efficient_finetuning_folder}/data/dataset_info.json\"\n",
    "\n",
    "    # Load the data_info.json file\n",
    "    with open(data_info_file, 'r') as f:\n",
    "        data_info = json.load(f)\n",
    "\n",
    "    # Create a new key by removing the .json extension from the file name\n",
    "    new_key = json_file_name.replace('.json', '')\n",
    "\n",
    "    # Add the new key to the data_info dictionary\n",
    "    data_info[new_key] = {\n",
    "        'file_name': json_file_name\n",
    "    }\n",
    "\n",
    "    # Save the updated data_info.json file\n",
    "    with open(data_info_file, 'w') as f:\n",
    "        json.dump(data_info, f, indent=4)\n",
    "\n",
    "    print(f'Added {new_key} to data_info.json')\n",
    "\n",
    "add_json_file(efficient_finetuning_folder, train)\n",
    "add_json_file(efficient_finetuning_folder, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ5J_uQb9wAi"
   },
   "source": [
    "- Run the following block to load the `generate_ftune_job` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "l1RmKZks8Moc"
   },
   "outputs": [],
   "source": [
    "def generate_ftune_job(account, cpus_per_task, mem, run_time, gpu, hf_token, efficient_finetuning_folder, stage, model_name_or_path, dataset, template, finetuning_type, lora_target, output_dir, per_device_train_batch_size, gradient_accumulation_steps, lr_scheduler_type, logging_steps, save_steps, learning_rate, num_train_epochs, plot_loss, fp16, filename):\n",
    "    text = f'''#!/bin/bash\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --time={run_time}\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:{gpu}\n",
    "\n",
    "module purge\n",
    "module load gcc/11.3.0\n",
    "module load python/3.9.12\n",
    "module load nvidia-hpc-sdk\n",
    "module load git/2.36.1\n",
    "module load cuda/11.8.0\n",
    "\n",
    "export HF_TOKEN={hf_token}\n",
    "huggingface-cli login --token $HF_TOKEN\n",
    "\n",
    "cd {efficient_finetuning_folder}\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\\\n",
    "    --stage {stage} \\\\\n",
    "    --model_name_or_path  {model_name_or_path} \\\\\n",
    "    --do_train \\\\\n",
    "    --dataset {dataset} \\\\\n",
    "    --template {template} \\\\\n",
    "    --finetuning_type {finetuning_type} \\\\\n",
    "    --lora_target {lora_target} \\\n",
    "    --output_dir {output_dir} \\\\\n",
    "    --overwrite_cache \\\\\n",
    "    --per_device_train_batch_size {per_device_train_batch_size} \\\\\n",
    "    --gradient_accumulation_steps {gradient_accumulation_steps} \\\\\n",
    "    --lr_scheduler_type {lr_scheduler_type} \\\\\n",
    "    --logging_steps {logging_steps} \\\\\n",
    "    --save_steps {save_steps} \\\\\n",
    "    --learning_rate {learning_rate} \\\\\n",
    "    --num_train_epochs {num_train_epochs} \\\\\n",
    "    {'--plot_loss' if plot_loss else ''}\\\\\n",
    "    {'--fp16' if fp16 else ''}'''\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwUYd8pr8QMR"
   },
   "source": [
    "- Update parameters of the generate_ftune_job function (if you are unsure about the values to use, leave them as they are or consult the Efficient Fine-Tuning repository or relevant literature for more information\n",
    "- After updating the parameters, run the following code to generate the job file to fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "cmzT7EqK8RxS"
   },
   "outputs": [],
   "source": [
    "generate_ftune_job(\n",
    "    account=\"ll_774_951\", #the account you are charging resources to\n",
    "    cpus_per_task=8, #default\n",
    "    mem=\"64GB\", #default\n",
    "    run_time=\"05:00:00\", #update accordingly to how much time your predictions take; it varies depending on the question length and resources chosen\n",
    "    gpu=\"a40\", #a40 or a100, a100 is faster but usually busy\n",
    "    hf_token=\"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\", #create a Hugging Face account and substitute with your token\n",
    "    efficient_finetuning_folder=\"/home1/pante/UntitledFolder/LLaMA-Efficient-Tuning\", #the cloned repo folder\n",
    "\n",
    "    stage=\"sft\", #default\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\", #you can change the model using the Hugging Face models\n",
    "    dataset=\"train\", #update with the name of the training dataset you uploaded\n",
    "    template=\"default\", #default\n",
    "    finetuning_type=\"lora\", #default\n",
    "    lora_target=\"q_proj,v_proj\", #default\n",
    "    output_dir=\"/home1/pante/UntitledFolder/train\", #select an output directory for your trained model, choose a different folder for every train dataset, be sure to select an empty folder\n",
    "    per_device_train_batch_size=4, #default\n",
    "    gradient_accumulation_steps=4, #default\n",
    "    lr_scheduler_type=\"cosine\", #default\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    learning_rate=\"5e-5\",\n",
    "    num_train_epochs=3.0,\n",
    "    plot_loss=True,\n",
    "    fp16=True,\n",
    "\n",
    "    filename=\"ftune.job\", #the name of the job file to start the fine-tuning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L30WxCga8TmR"
   },
   "source": [
    "- Run the following code to start the job for fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "L4j2VVfV8aRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16604771\n"
     ]
    }
   ],
   "source": [
    "!sbatch ftune.job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUS-LBgy8YNo"
   },
   "source": [
    "Check your job at [https://ondemand.carc.usc.edu/pun/sys/dashboard/activejobs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmUTonq68c6Z"
   },
   "source": [
    "### Prediction:\n",
    "- Update the parameters.\n",
    "- Run the following code blocks to start prediction.\n",
    "- The results will be in the output folder you selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "LSFrcnaX8UQL"
   },
   "outputs": [],
   "source": [
    "def generate_predict_job(account, cpus_per_task, mem, run_time, gpu, hf_token, efficient_finetuning_folder,dataset,train_dir,output_dir, stage, model_name_or_path, template, finetuning_type, per_device_eval_batch_size, max_samples, filename):\n",
    "    text = f'''#!/bin/bash\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --time={run_time}\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:{gpu}\n",
    "\n",
    "module purge\n",
    "module load gcc/11.3.0\n",
    "module load python/3.9.12\n",
    "module load nvidia-hpc-sdk\n",
    "module load git/2.36.1\n",
    "module load cuda/11.8.0\n",
    "\n",
    "export HF_TOKEN={hf_token}\n",
    "huggingface-cli login --token $HF_TOKEN\n",
    "\n",
    "cd {efficient_finetuning_folder}\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "dataset={dataset}\n",
    "output_dir={output_dir}\n",
    "\n",
    "echo \"########### PREDICT_FT.JOB ############\"\n",
    "echo \"dataset: $dataset\"\n",
    "echo \"output_dir: $output_dir\"\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\\\n",
    "    --stage {stage} \\\\\n",
    "    --model_name_or_path  {model_name_or_path} \\\\\n",
    "    --do_predict \\\\\n",
    "    --dataset {dataset} \\\\\n",
    "    --template {template} \\\\\n",
    "    --finetuning_type {finetuning_type} \\\\\n",
    "    --checkpoint_dir {train_dir} \\\\\n",
    "    --output_dir {output_dir}\\\\\n",
    "    --per_device_eval_batch_size {per_device_eval_batch_size} \\\\\n",
    "    --max_samples {max_samples} \\\\\n",
    "    --predict_with_generate'''\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xw6WkFvk8fHr"
   },
   "outputs": [],
   "source": [
    "generate_predict_job(\n",
    "    account=\"ll_774_951\", #the account you are charging resources to\n",
    "    cpus_per_task=8, #default\n",
    "    mem=\"64GB\", #default\n",
    "    run_time=\"03:00:00\", #update accordingly to how much time your predictions take; it varies depending on the question length and resources chosen\n",
    "    gpu=\"a40\", #a40 or a100, a100 is faster but usually busy\n",
    "    hf_token=\"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\", #create a Hugging Face account and substitute with your token\n",
    "    efficient_finetuning_folder=\"/home1/pante/UntitledFolder/LLaMA-Efficient-Tuning\", #the cloned repo folder\n",
    "    dataset=\"test\",\n",
    "    train_dir=\"/home1/pante/UntitledFolder/train\",\n",
    "    output_dir=\"/home1/pante/UntitledFolder/test\", # be sure to select an empty folder\n",
    "    stage=\"sft\", #default\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\", #you can change the model using the Hugging Face models\n",
    "    template=\"default\", #default\n",
    "    finetuning_type=\"lora\", #default\n",
    "    per_device_eval_batch_size=8, #default\n",
    "    max_samples=10000,\n",
    "    filename=\"predict.job\" #the name of the job file to start the prediction\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nF6pVmlQ8g2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16604894\n"
     ]
    }
   ],
   "source": [
    "!sbatch predict.job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i-xOHve8ix0"
   },
   "source": [
    "Check your job at [https://ondemand.carc.usc.edu/pun/sys/dashboard/activejobs]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
